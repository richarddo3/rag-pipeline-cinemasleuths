from .ingest import load_csv_documents, load_directory_texts, chunk_documents
from .embeddings import embed_texts
from .vector_store import build_faiss_index
from .retriever import retrieve_top_k

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import numpy as np


# -----------------------------
# Local LLM (Qwen 1.5 ‚Äì 4B)
# -----------------------------
_tokenizer = None
_model = None

def get_llm():
    global _tokenizer, _model

    if _model is None:
        model_name = "Qwen/Qwen1.5-4B-Chat"
        _tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        _model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

    return _tokenizer, _model


# -----------------------------
# Build RAG Pipeline
# -----------------------------
def build_rag_pipeline(
    csv_path="data/etl_cleaned_dataset.csv",
    extra_docs_dir="data/additional_documents"
):
    print("üìÑ Loading CSV documents...")
    docs = load_csv_documents(csv_path)

    if extra_docs_dir:
        print("üìÇ Loading additional documents...")
        docs += load_directory_texts(extra_docs_dir)

    print("‚úÇÔ∏è Chunking documents...")
    docs = chunk_documents(docs, chunk_size=1000, chunk_overlap=200)
    docs = [d for d in docs if isinstance(d, dict)]

    print("üß† Generating embeddings...")
    texts = [d["text"] for d in docs]
    embeddings = embed_texts(texts)

    # Normalize for cosine similarity
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    print("üì¶ Building FAISS index...")
    index = build_faiss_index(embeddings)

    print("‚úÖ RAG pipeline ready.")
    return {
        "index": index,
        "documents": docs
    }


# -----------------------------
# Answer Question
# -----------------------------
def answer_question(pipeline, question, k=4):
    docs = pipeline["documents"]
    index = pipeline["index"]

    # --- Retrieve ---
    retrieved = retrieve_top_k(index, docs, question, k=k)

    if not retrieved:
        return {
            "answer": "I cannot answer from the provided data.",
            "sources": []
        }

    # Build context
    context = "\n\n".join(
        f"[SOURCE: {r['id']}]\n{r['text']}"
        for r in retrieved
    )

    # --- Prompt ---
    prompt = f"""
    You are a movie data assistant.
    
    Answer using ONLY the context below.
    Compare values carefully and choose the correct answer.
    Cite the source ID(s) you used.
    
    If the answer is not present, say:
    "I cannot answer from the provided data."
    
    Question:
    {question}
    
    Context:
    {context}
    
    Answer (be concise and factual):
    """

    # --- Generate ---
    tokenizer, model = get_llm()

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=120,
        do_sample=False
    )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # --- Format sources ---
    sources = []
    for r in retrieved:
        snippet = r["text"][:200].replace("\n", " ")
        sources.append({
            "id": r["id"],
            "source": "etl_cleaned_dataset.csv",
            "snippet": snippet
        })

    return {
        "answer": answer.strip(),
        "sources": sources
    }
